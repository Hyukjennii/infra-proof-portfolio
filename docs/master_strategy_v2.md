# 🎯 Master Strategy v2: 데이터 인프라 실무 역량 증빙 프로젝트

---

## 1️⃣ 프로젝트 개요
본 프로젝트는 회사 데이터를 사용하지 않고,  
개인 VM 환경에서 **인프라 실무 역량을 객관적·정량적으로 증빙**하기 위한 실험형 포트폴리오이다.

주요 목표:
- 백업, 복구, 네트워크, 자동화 등 핵심 인프라 영역을 재현
- 데이터 안정성과 복원력, 운영 효율화 능력을 실험 기반으로 입증
- 상용 백업 환경(Zconverter, NetWorker)과 동일한 구조를 **무료 오픈소스(Bareos 등)** 로 구현
- 모든 실험과 분석은 **가명화·비식별화** 처리로 보안 문제 완전 회피

---

## 2️⃣ 커리어 방향 (상세 절차 포함)
- **현재 상태:** 방통대 통계데이터학과 1학기 재학, 현직 인프라 담당자  
- **최종 목표:** 2027년 상반기 **데이터 인프라 / MLOps 엔지니어 이직**

---

### 🎯 커리어 3단계 로드맵 (2025.10 ~ 2027.06)

#### **① 1단계: 기반 강화기 (2025.10 ~ 2026.03)**
> 핵심 역량: 시스템 인프라 + Python 자동화

| 세부 목표 | 실행 내용 | 검증 방법 |
|------------|------------|------------|
| OS/네트워크 심화 | Linux 성능관리, Shell 스크립트, Cron 자동화 | `check_disk.sh`, `check_network.sh` 리포트화 |
| 백업 구조 이해 | rsync, Bareos 기반 백업/복구 실험 | 로그 수집 → Python 파싱 → CSV 분석 |
| Python 실무화 | Pandas, Matplotlib 기반 통계 처리 | 백업 통계 리포트 자동 생성 |
| 기초 통계 응용 | 학습 과제 + 로그 데이터 분석 연계 | 학기별 과제 → 인프라 데이터 적용 |

🧩 **성과물:**  
- `infra-proof-portfolio` 초기 구축  
- 3대 실험(백업·장애·효율) 리포트 1차 완성  

---

#### **② 2단계: 데이터 인프라 전환기 (2026.04 ~ 2026.12)**
> 핵심 역량: 데이터 파이프라인 + 분산 환경 운영

| 세부 목표 | 실행 내용 | 검증 방법 |
|------------|------------|------------|
| 분산 처리 실습 | Hadoop, Spark 로컬 클러스터 구성 | VM 내 Spark Job 실행 |
| 실시간 데이터 흐름 | Kafka, Flink 로그 스트림 구축 | 스트림 안정성 실험 |
| 데이터 저장 최적화 | ClickHouse, StarRocks 구축 | OLAP 질의 속도 비교 |
| 자동화 확장 | Airflow DAG 기반 ETL 구성 | 주간 배치 파이프라인 완성 |

🧩 **성과물:**  
- `data-pipeline-lab` 구축  
- Spark/Kafka 기반 파이프라인 리포트 2종  
- 인프라→데이터 전이 사례 문서화  

---

#### **③ 3단계: 완성 및 이직기 (2027.01 ~ 2027.06)**
> 핵심 역량: 통합·자동화·최적화 + 이직 포트폴리오 완성

| 세부 목표 | 실행 내용 | 검증 방법 |
|------------|------------|------------|
| 통합 시나리오 | 백업+장애+데이터파이프라인 통합 | End-to-End 리포트 작성 |
| 자동 보고 체계 | Notebook → PDF 자동화 | `generate_reports.py` |
| 포트폴리오 구성 | README + 기술 스택 매트릭스 | 깃허브/노션 공개 |
| 면접 대비 | 복구·성능·자동화 사례 중심 Q&A | 리포트 기반 질의응답 준비 |

🧩 **성과물:**  
- 최종 포트폴리오 패키지 완성  
- “복원력·자동화·효율성” 실적 리포트 확보  
- 2027 상반기 이직 실행

---

## 3️⃣ 기술 역량 구분
주인님의 현재 강점과 미래 목표를 명확히 구분하여 구성했습니다.

| 구분 | 목적 | 기술 스택 | 결과물 | 검증 방법 |
|------|------|------------|----------|------------|
# 🎯 Master Strategy v2: 데이터 인프라 실무 역량 증빙 프로젝트

---

## 1️⃣ 프로젝트 개요
본 프로젝트는 회사 데이터를 사용하지 않고,  
개인 VM 환경에서 **인프라 실무 역량을 객관적·정량적으로 증빙**하기 위한 실험형 포트폴리오이다.

주요 목표:
- 백업, 복구, 네트워크, 자동화 등 핵심 인프라 영역을 재현
- 데이터 안정성과 복원력, 운영 효율화 능력을 실험 기반으로 입증
- 상용 백업 환경(Zconverter, NetWorker)과 동일한 구조를 **무료 오픈소스(Bareos 등)** 로 구현
- 모든 실험과 분석은 **가명화·비식별화** 처리로 보안 문제 완전 회피

---

## 2️⃣ 커리어 방향 (상세 절차 포함)
- **현재 상태:** 방통대 통계데이터학과 1학기 재학, 현직 인프라 담당자  
- **최종 목표:** 2027년 상반기 **데이터 인프라 / MLOps 엔지니어 이직**

---

### 🎯 커리어 3단계 로드맵 (2025.10 ~ 2027.06)

#### **① 1단계: 기반 강화기 (2025.10 ~ 2026.03)**
> 핵심 역량: 시스템 인프라 + Python 자동화

| 세부 목표 | 실행 내용 | 검증 방법 |
|------------|------------|------------|
| OS/네트워크 심화 | Linux 성능관리, Shell 스크립트, Cron 자동화 | `check_disk.sh`, `check_network.sh` 리포트화 |
| 백업 구조 이해 | rsync, Bareos 기반 백업/복구 실험 | 로그 수집 → Python 파싱 → CSV 분석 |
| Python 실무화 | Pandas, Matplotlib 기반 통계 처리 | 백업 통계 리포트 자동 생성 |
| 기초 통계 응용 | 학습 과제 + 로그 데이터 분석 연계 | 학기별 과제 → 인프라 데이터 적용 |

🧩 **성과물:**  
- `infra-proof-portfolio` 초기 구축  
- 3대 실험(백업·장애·효율) 리포트 1차 완성  

---

#### **② 2단계: 데이터 인프라 전환기 (2026.04 ~ 2026.12)**
> 핵심 역량: 데이터 파이프라인 + 분산 환경 운영

| 세부 목표 | 실행 내용 | 검증 방법 |
|------------|------------|------------|
| 분산 처리 실습 | Hadoop, Spark 로컬 클러스터 구성 | VM 내 Spark Job 실행 |
| 실시간 데이터 흐름 | Kafka, Flink 로그 스트림 구축 | 스트림 안정성 실험 |
| 데이터 저장 최적화 | ClickHouse, StarRocks 구축 | OLAP 질의 속도 비교 |
| 자동화 확장 | Airflow DAG 기반 ETL 구성 | 주간 배치 파이프라인 완성 |

🧩 **성과물:**  
- `data-pipeline-lab` 구축  
- Spark/Kafka 기반 파이프라인 리포트 2종  
- 인프라→데이터 전이 사례 문서화  

---

#### **③ 3단계: 완성 및 이직기 (2027.01 ~ 2027.06)**
> 핵심 역량: 통합·자동화·최적화 + 이직 포트폴리오 완성

| 세부 목표 | 실행 내용 | 검증 방법 |
|------------|------------|------------|
| 통합 시나리오 | 백업+장애+데이터파이프라인 통합 | End-to-End 리포트 작성 |
| 자동 보고 체계 | Notebook → PDF 자동화 | `generate_reports.py` |
| 포트폴리오 구성 | README + 기술 스택 매트릭스 | 깃허브/노션 공개 |
| 면접 대비 | 복구·성능·자동화 사례 중심 Q&A | 리포트 기반 질의응답 준비 |

🧩 **성과물:**  
- 최종 포트폴리오 패키지 완성  
- “복원력·자동화·효율성” 실적 리포트 확보  
- 2027 상반기 이직 실행

---

## 3️⃣ 기술 역량 구분
주인님의 현재 강점과 미래 목표를 명확히 구분하여 구성했습니다.

| 구분 | 목적 | 기술 스택 | 결과물 | 검증 방법 |
|------|------|------------|----------|------------|
| **현재 업무: 인프라 운영 (infra_ops)** | 안정적 운영, 자동화, 복구 | Linux, Shell, Bareos, Python, Cron | 백업·복구 리포트 | 성공률·효율 개선율 |
| **전환 목표: 데이터 인프라 (data_infra)** | 데이터 파이프라인 및 분산처리 | Kafka, Spark, Airflow, ClickHouse | ETL 및 스트림 리포트 | 처리속도·지연율 분석 |

---

## 4️⃣ 포트폴리오 구조 (주석 포함)


infra-proof-portfolio/
├── infra_ops/ # ✅ 현재 업무 기반 (Existing Competency)
│ ├── scripts/ # 운영 자동화 스크립트
│ │ ├── check_disk.sh # 디스크 상태 점검
│ │ ├── check_network.sh # 네트워크 진단 및 응답시간 측정
│ │ ├── alert_notify.sh # 경보 테스트 (Slack/Webhook 연동)
│ │ └── cron_jobs.txt # 정기 점검 스케줄 목록
│ ├── reports/ # 운영 성과 리포트
│ │ ├── backup_summary.md # 백업 성공률/소요시간 분석
│ │ ├── operation_efficiency_gain.md # 자동화 전후 효율 비교
│ │ └── fault_recovery.md # 장애 복원 사례 리포트
│ ├── data/ # 로그 및 변환 데이터
│ │ ├── backup_sim.log # 백업 로그 샘플
│ │ └── parsed_backup.csv # Python 분석용 CSV
│ └── notebooks/ # Python 분석 노트북
│ └── backup_trend.ipynb # 백업 추세 및 통계 시각화

├── data_infra/ # 🚀 커리어 전환 목표 (New Competency)
│ ├── pipelines/ # 데이터 파이프라인 구축
│ │ ├── kafka_log_streamer/ # Kafka 기반 로그 수집
│ │ ├── spark_log_analyzer/ # Spark 분산 처리 분석
│ │ └── airflow_daily_jobs/ # Airflow ETL 자동화
│ ├── notebooks/
│ │ ├── spark_analysis.ipynb # Spark Job 성능 분석
│ │ ├── performance_summary.ipynb # 통합 성능 대시보드
│ │ └── error_trends.ipynb # 에러 추세 분석
│ ├── reports/
│ │ ├── data_pipeline_performance.md # 파이프라인 성능 비교
│ │ └── spark_scalability_test.md # 확장성 테스트 결과
│ └── dags/
│ └── backup_etl.py # Airflow DAG (ETL 파이프라인)

├── docs/ # 📘 전략 및 정책 문서
│ ├── master_strategy_v2.md # 본 문서 (통합 백서)
│ ├── roadmap_2025-2027.md # 연도별 로드맵 세부 내용
│ ├── infra_topology.png # VM 및 네트워크 구성도
│ ├── backup_schedule.md # 백업 정책 및 보존 주기
│ ├── fault_recovery_workflow.md # 장애 대응 프로세스
│ └── adsp_summary.md # ADsP 학습 정리

├── automation/ # 🤖 공통 자동화 및 CI 레이어
│ ├── generate_reports.py # Markdown → PDF 자동 변환 스크립트
│ └── report_templates/ # 리포트 템플릿 저장소

└── README.md # 프로젝트 요약 / 구조 안내 / 링크 집약
